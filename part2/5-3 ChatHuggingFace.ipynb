{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-3 ChatHuggingFace를 활용한 LLM 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonkang/personal/langgraph-book/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct:\n",
      "- configuration_exaone.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct:\n",
      "- modeling_exaone.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Fetching 7 files:   0%|          | 0/7 [00:11<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 import\n",
    "import torch  # 디바이스(CPU/GPU) 확인용\n",
    "# 허깅페이스의 모델을 파이프라인 형태로 불러와서 사용할 수 있다.\n",
    "# ChatHuggingFace는 HuggingFacePipeline을 chat 형태로 감싸주는 wrapper 클래스다.\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline  # HuggingFace 파이프라인 및 채팅 래퍼\n",
    "\n",
    "# 디바이스 설정: GPU가 있으면 0, 없으면 -1(CPU)\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# 허깅페이스 허브에서 모델을 파이프라인 형태로 불러오기\n",
    "# task를 'text-generation'으로 지정해서 텍스트 생성용 파이프라인을 만든다.\n",
    "chat_model = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "      max_new_tokens=1024,  # 생성할 최대 토큰 수\n",
    "      do_sample=False,     # 샘플링 비활성화(결정적 생성)\n",
    "      repetition_penalty=1.03,  # 반복 패널티\n",
    "    ),\n",
    "    device=device  # 0 for first GPU, -1 for CPU\n",
    ")\n",
    "\n",
    "# 생성된 파이프라인을 ChatHuggingFace에 전달해서 LangChain 문법을 활용할 수 있다.\n",
    "llm = ChatHuggingFace(llm=chat_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='RAG(Retrieval-Augmented Generation) 파이프라인은 자연어 처리(NLP) 분야에서 최근 주목받고 있는 모델 아키텍처입니다. 이 접근 방식은 특히 대화형 AI, 정보 검색 및 생성 작업에 유용합니다. RAG는 두 가지 주요 컴포넌트로 구성됩니다:\\n\\n1. Retrieval 모듈:\\n   - 외부 지식베이스나 문서 집합에서 필요한 정보를 검색하고 추출하는 역할을 합니다.\\n   - 사용자의 질문이나 입력에 대해 관련된 문서나 데이터를 빠르게 찾아냅니다.\\n\\n2. Generation 모듈:\\n   - 검색된 정보를 바탕으로 텍스트를 생성하거나 완성하는 역할을 합니다.\\n   - GPT-3와 같은 대형 언어 모델을 사용하여 문맥에 맞는 답변이나 텍스트를 생성합니다.\\n\\nRAG 파이프라인의 작동 방식:\\n\\n1. 사용자가 질문이나 입력을 제공하면,\\n2. Retrieval 모듈이 해당 질문과 관련된 정보를 데이터베이스 또는 문서 집합에서 검색합니다.\\n3. 검색된 정보는 Generation 모듈로 전달되며,\\n4. Generation 모듈은 이를 바탕으로 적절한 답변이나 텍스트를 생성합니다.\\n\\n이 과정을 통해 RAG 시스템은 더 정확하고 풍부한 응답을 제공할 수 있습니다. 예를 들어, 챗봇이 사용자의 질문에 대해 보다 신뢰성 있는 답변을 제공하기 위해 최신 뉴스 기사나 전문 자료를 참조할 수 있습니다.\\n\\nRAG는 기존의 언어 모델보다 더 높은 품질의 응답을 생성할 수 있도록 도와주며, 특히 정보의 정확성과 관련성이 중요한 응용 분야에서 큰 장점을 가집니다.')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"RAG 파이프라인은 무엇인가요?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
